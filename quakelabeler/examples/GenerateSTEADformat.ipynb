{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd6eb316-67e5-4d39-b113-282f2b2f8127",
   "metadata": {},
   "source": [
    "# Convert Local Data to an AI training Set\n",
    "## Example: From USGS dataset to STEAD(HDF5 + CSV)format\n",
    "Author: Hao Mai \n",
    "\n",
    "E-mail: hmai090@uottawa.ca\n",
    "\n",
    "Oct 04 2022\n",
    "\n",
    "Here is USGS dataset for first arriving P,Pg,Pn,S,Sg,Sn from the PDE (2013+):\n",
    "https://www.sciencebase.gov/catalog/item/6127b30fd34e40dd9c05094c\n",
    "You can download one month zip file to try following codes, e.g., [2013 June zip](https://www.sciencebase.gov/catalog/item/61fc238ad34e622189cb4240) is the smallest size.\n",
    "\n",
    "-In general, a public dataset will offer user a csv file which includes all metadata, i.e., catalog / event related informations.\n",
    "-To deploy a AI-based phase picking projects, in most time, we do not need that much information, we only concern a few of them.\n",
    "-Most important thing is we need to trim the original seismograms(traces) to the same format in model, e.g., STEAD format is one of the most popular used one.\n",
    "Okay, Let's start to build it!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d1f015-fbf9-4e21-8376-b07f85f9f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import csv\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "from tqdm import tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9262b9-bff3-4aaa-b2d1-7c9d77e119be",
   "metadata": {},
   "source": [
    "## rewirte_trace_name function\n",
    "`rewirte_trace_name` Rewrite the trace name to the format of the STEAD csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2220ed6d-0265-4106-ba17-ae634436fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewirte_trace_name(tr):\n",
    "    \"\"\"\n",
    "    Rewrite the trace name to the format of the STEAD csv file\n",
    "    :param tr: DataFrame Series\n",
    "    :return: trace_name: str\n",
    "    \"\"\"\n",
    "    # sample format for trace_name\n",
    "    # USGS.109c.TA_20061101031222459_EV\n",
    "    # 16 digits phasetime + last random digit avoid duplicate\n",
    "    time = ''.join(re.findall(r'\\d+', tr.phase_time))\n",
    "    time = str(time[:16])\n",
    "    time = time + str(np.random.randint(0, 9))\n",
    "    trace_name = \"USGS.\" + str(tr.station) + \".\" + str(tr.network) + \"_\" + time + \"_EV\"\n",
    "    return trace_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5939f322-f48e-495b-bcec-5727942ea9ff",
   "metadata": {},
   "source": [
    "## rewirte_arrivals function\n",
    "`rewirte_arrivals` return all find arrival time and convert them to position in the trace (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b602007-0fdd-4151-8d31-f7560c7ed19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewirte_arrivals(index):\n",
    "    \"\"\"\n",
    "    Rewrite the arrival time to the format of the STEAD csv file\n",
    "    :param i: index\n",
    "    :return: arrivals: Numpy array, e.g., p_pn_pg_s_sn_sg=[500, None, 550, 1000, None, None]\n",
    "\n",
    "    \"\"\"\n",
    "    global usgs_df\n",
    "    arr_dict = {\"P\": np.NaN, \"Pn\": np.NaN, \"Pg\": np.NaN, \"S\": np.NaN, \"Sn\": np.NaN, \"Sg\": np.NaN}\n",
    "    phasetime = UTCDateTime(usgs_df.loc[index].phase_time)\n",
    "    nscl_id = usgs_df.loc[index].nscl_code\n",
    "    df_same_sta = usgs_df[usgs_df.nscl_code == nscl_id]\n",
    "    # USGS dataset the original arrival is set at 60 sec which is 60 * 40 (sample_rate) = 2400\n",
    "    # if arrival is at other position, change here\n",
    "    arr = 2400\n",
    "    arr_dict[usgs_df.loc[index].phase_hint] = arr\n",
    "    # find the arrival time of other phases in same trace\n",
    "    # in USGS catalog, other phases are list at different rows\n",
    "    # the following code is to find the arrival time of other phases in same trace\n",
    "    for inx, new_row in df_same_sta.iterrows():\n",
    "        if inx == index:\n",
    "            continue\n",
    "        new_phase_time = UTCDateTime(usgs_df.loc[inx].phase_time)\n",
    "        if abs(new_phase_time - phasetime) <= 60:\n",
    "            new_arr = arr + int((new_phase_time - phasetime) * 40)\n",
    "            arr_dict[usgs_df.loc[inx].phase_hint] = new_arr\n",
    "            usgs_df = usgs_df.drop(inx)\n",
    "    arrivals = list(arr_dict.values())\n",
    "    # return all find arrival time and convert them to position in the trace (int)\n",
    "    return arrivals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ccb48-3b9f-454c-bd60-e6890d5022af",
   "metadata": {},
   "source": [
    "## rebuild_csv function\n",
    "`rebuild_csv` generate a new csv file for metadata(information), a HDF5 file for training use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c5f1173-3244-47bc-8e08-aabe5da546de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_csv(csv_path, csv_name='data.csv'):\n",
    "    \"\"\"\n",
    "    Rebuild the csv file from the original csv file, i.e. USGS\n",
    "    csv_path: the path of the original csv file\n",
    "    csv_name: the name of the new csv file\n",
    "    \"\"\"\n",
    "    # define hdf5 file name\n",
    "    output_merge = csv_name[:-4] + '.hdf5'\n",
    "    # create hdf5 file\n",
    "    HDF0 = h5py.File(output_merge, 'a')\n",
    "    HDF0.create_group(\"data\")\n",
    "    # read csv file\n",
    "    global usgs_df\n",
    "    usgs_df = pd.read_csv(csv_path)\n",
    "    h5name = csv_path[:-4] + '.h5'\n",
    "    dtfl = h5py.File(h5name, 'r')\n",
    "\n",
    "    csvfile = open(csv_name, 'w')\n",
    "    output_writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    # add an additional phase type (i.e. P vs Pn vs Pg etc)\n",
    "    # label options\n",
    "    # labels = ['network_code','receiver_code','receiver_type','receiver_latitude','receiver_longitude',\n",
    "    #                              'receiver_elevation_m','phase_type','p_arrival_sample','p_status','p_weight','p_travel_sec',\n",
    "    #                              's_arrival_sample','s_status','s_weight',\n",
    "    #                              'source_id','source_origin_time','source_origin_uncertainty_sec',\n",
    "    #                              'source_latitude','source_longitude','source_error_sec',\n",
    "    #                              'source_gap_deg','source_horizontal_uncertainty_km', 'source_depth_km', 'source_depth_uncertainty_km',\n",
    "    #                              'source_magnitude', 'source_magnitude_type', 'source_magnitude_author','source_mechanism_strike_dip_rake',\n",
    "    #                              'source_distance_deg', 'source_distance_km', 'back_azimuth_deg', 'snr_db', 'coda_end_sample',\n",
    "    #                              'trace_start_time', 'trace_category', 'trace_name']\n",
    "    # Adaptive label options\n",
    "    # p_pn_pg_s_sn_sg : list, arrival sample points of P, Pn, Pg, S, Sn, Sg phase\n",
    "    # [500, None, 550, 1000, None, None]  value is None if no arrival, value should be integer\n",
    "    picklabels = ['p_pn_pg_s_sn_sg', 'snr_db', 'source_magnitude',\n",
    "                  'receiver_type', 'source_distance_deg', 'source_distance_km',\n",
    "                  'trace_category', 'trace_name', 'source_id'\n",
    "                  ]\n",
    "    output_writer.writerow(picklabels)\n",
    "    csvfile.flush()\n",
    "    print('output files have been generated!')\n",
    "    # label of trace_category in the csv file\n",
    "    trace_category = 'earthquake'  # earthquake label\n",
    "    # main loop for each file (trace)\n",
    "    with tqdm(total=len(usgs_df)) as pbar:\n",
    "        for i in range(len(usgs_df)):\n",
    "            pbar.update(1)\n",
    "            if i not in usgs_df.index:\n",
    "                continue\n",
    "            try:\n",
    "                trace_name = rewirte_trace_name(usgs_df.loc[i])\n",
    "                p_pn_pg_s_sn_sg = rewirte_arrivals(i)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(usgs_df.loc[i])\n",
    "                print(i)\n",
    "\n",
    "            output_writer.writerow([p_pn_pg_s_sn_sg,\n",
    "                                    usgs_df.loc[i].snr_db,\n",
    "                                    usgs_df.loc[i].source_magnitude,\n",
    "                                    usgs_df.loc[i].channel[:2],  # receiver_type\n",
    "                                    usgs_df.loc[i].source_distance_deg,  # float64\n",
    "                                    usgs_df.loc[i].source_distance_km,  # float64\n",
    "                                    trace_category,  # trace_category\n",
    "                                    trace_name,  # trace_name\n",
    "                                    usgs_df.loc[i].phase_id,  # source_id\n",
    "                                    ])\n",
    "            # save the trace data to hdf5 file\n",
    "            x = dtfl.get(usgs_df.loc[i].event_id + '/' + usgs_df.loc[i].waves_id + '/' + usgs_df.loc[i].phase_id)\n",
    "            data = np.array(x)\n",
    "            # write metadata (attributes)\n",
    "            HDFr = h5py.File(output_merge, 'a')\n",
    "            dsF = HDFr.create_dataset(\"data/\" + trace_name, data.shape, data=data, dtype=np.float32)\n",
    "            dsF.attrs['p_pn_pg_s_sn_sg'] = np.array(p_pn_pg_s_sn_sg)\n",
    "            dsF.attrs['snr_db'] = usgs_df.loc[i].snr_db\n",
    "            dsF.attrs['source_magnitude'] = usgs_df.loc[i].source_magnitude\n",
    "            dsF.attrs['receiver_type'] = usgs_df.loc[i].channel[:2]\n",
    "            dsF.attrs['source_distance_deg'] = usgs_df.loc[i].source_distance_deg\n",
    "            dsF.attrs['source_distance_km'] = usgs_df.loc[i].source_distance_km\n",
    "            dsF.attrs['trace_category'] = trace_category\n",
    "            dsF.attrs['trace_name'] = trace_name\n",
    "            dsF.attrs['source_id'] = usgs_df.loc[i].phase_id\n",
    "            HDFr.flush()\n",
    "    HDFr.close()\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4cedb7d-2721-4c0e-80ee-8984c1bd94cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file_path='/Users/hao/Downloads/2013/'):\n",
    "    \"\"\"\n",
    "    Convert dataset into HDF5\n",
    "    file_path: directory of csv files \n",
    "    \"\"\"\n",
    "    time_start = time.time()\n",
    "    file_path = file_path + '*.csv'\n",
    "    # finished = ['/Users/hao/Downloads/2013/201309_mlaapde.csv']\n",
    "    # finished: complete csv file\n",
    "    for csv_file in glob.glob(file_path):\n",
    "        if csv_file in finished:\n",
    "            continue\n",
    "        print('working on ' + csv_file + '...')\n",
    "        title = ''.join(re.findall(r'\\d+', csv_file)[0])\n",
    "        # write uniform new csv name\n",
    "        title = 'usgs'+ title + '.csv'\n",
    "        rebuild_csv(csv_file, title)\n",
    "    print('Done!')\n",
    "    time_end = time.time()\n",
    "    print('Total time cost: ', (time_end - time_start)/60, 'minutes')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96cf4c9-2c95-45f4-8be1-70a59a03bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # input csv file path and put data files in the same directory\n",
    "    main('/Volumes/TOSHIBA EXT/STEAD/USGS/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
